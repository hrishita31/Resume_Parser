{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Data Dazzlersâœ¨**\n"
      ],
      "metadata": {
        "id": "UJTrzy6rdB2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCpCn_ttCqVD",
        "outputId": "69c92739-39a9-4446-b8dd-9a8f87a0abe5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-pptx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHK74LGMuMsM",
        "outputId": "b21677ca-fcb5-4726-b179-04ac5891fba1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-pptx in /usr/local/lib/python3.10/dist-packages (0.6.23)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show python-pptx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7f8n73_uQyR",
        "outputId": "2e3ad2de-aebf-4b16-9b89-936d56bf7caa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: python-pptx\n",
            "Version: 0.6.23\n",
            "Summary: Generate and manipulate Open XML PowerPoint (.pptx) files\n",
            "Home-page: https://github.com/scanny/python-pptx\n",
            "Author: Steve Canny\n",
            "Author-email: python-pptx@googlegroups.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: lxml, Pillow, XlsxWriter\n",
            "Required-by: textract\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFbGG_iHuSOK",
        "outputId": "7c10e73c-9a25-49c6-b546-234ea18ba7b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNUsHjQeUXbS",
        "outputId": "688cdcc9-ae3f-4b22-8e02-63c6cef71d12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqEmypvha03O",
        "outputId": "88bfd10a-2936-4a7c-b54f-b2847c118b1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textract in /usr/local/lib/python3.10/dist-packages (1.6.5)\n",
            "Requirement already satisfied: argcomplete~=1.10.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.10.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.8.0 in /usr/local/lib/python3.10/dist-packages (from textract) (4.8.2)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from textract) (0.8)\n",
            "Requirement already satisfied: extract-msg<=0.29.* in /usr/local/lib/python3.10/dist-packages (from textract) (0.28.7)\n",
            "Requirement already satisfied: pdfminer.six==20191110 in /usr/local/lib/python3.10/dist-packages (from textract) (20191110)\n",
            "Requirement already satisfied: python-pptx~=0.6.18 in /usr/local/lib/python3.10/dist-packages (from textract) (0.6.23)\n",
            "Requirement already satisfied: six~=1.12.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.12.0)\n",
            "Requirement already satisfied: SpeechRecognition~=3.8.1 in /usr/local/lib/python3.10/dist-packages (from textract) (3.8.1)\n",
            "Requirement already satisfied: xlrd~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (3.20.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Requirement already satisfied: imapclient==2.1.0 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (0.47)\n",
            "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.2)\n",
            "Requirement already satisfied: compressed-rtf>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
            "Requirement already satisfied: ebcdic>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2mNDypgRM93",
        "outputId": "ab1dcb89-1248-4dcb-b4ea-ae50b021b015"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pptx\n",
        "from bs4 import BeautifulSoup\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import textract\n",
        "from docx import Document"
      ],
      "metadata": {
        "id": "TfEXdmQRBH3h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "     text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_ppt(file_path):\n",
        "    text = \"\"\n",
        "    ppt = python_pptx.Presentation(file_path)\n",
        "    for slide in ppt.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def extract_text_from_html(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        html_content = file.read()\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        return soup.get_text()\n",
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    image = Image.open(file_path)\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# def extract_text_from_doc(file_path):\n",
        "#     doc = docx.Document(file_path)\n",
        "#     text = \"\"\n",
        "#     for paragraph in doc.paragraphs:\n",
        "#         text += paragraph.text + \"\\n\"\n",
        "#     return text\n",
        "def extract_text_from_doc(file_path):\n",
        "    text = textract.process(file_path).decode(\"utf-8\")\n",
        "    return text\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    if file_extension.lower() == '.pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_extension.lower() == '.pptx':\n",
        "        return extract_text_from_ppt(file_path)\n",
        "    elif file_extension.lower() == '.html':\n",
        "        return extract_text_from_html(file_path)\n",
        "    elif file_extension.lower() in ['.png', '.jpg', '.jpeg']:\n",
        "        return extract_text_from_image(file_path)\n",
        "    elif file_extension.lower() == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    # elif file_extension.lower() == '.doc':\n",
        "    #     return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        return \"Unsupported file format\""
      ],
      "metadata": {
        "id": "bvgpZ_tyt9IZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def list_files_in_directory(directory):\n",
        "    files = os.listdir(directory)\n",
        "    return files\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def identify_sections(resume_text):\n",
        "    # Preprocess the resume text\n",
        "    resume_text = preprocess_text(resume_text)\n",
        "\n",
        "    # Define regular expressions for common section headers\n",
        "    section_headers_regex = {\n",
        "        'education': r'\\b(education|academic qualifications)\\b',\n",
        "        'experience': r'\\b(experience|work experience|professional experience)\\b',\n",
        "        'skills': r'\\b(skills|technical skills)\\b',\n",
        "        'projects': r'\\b(projects|personal projects|side projects)\\b',\n",
        "        # Add more section headers as needed\n",
        "    }\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    # Iterate over each section header regex\n",
        "    for section, regex_pattern in section_headers_regex.items():\n",
        "        # Search for the section header in the preprocessed resume text\n",
        "        match = re.search(regex_pattern, resume_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            # Extract the section content starting from the position of the match\n",
        "            section_start_index = match.end()\n",
        "            # Find the end of the section or the start of the next section\n",
        "            next_section_start = len(resume_text)\n",
        "            for next_section in section_headers_regex.values():\n",
        "                next_match = re.search(next_section, resume_text[section_start_index:], re.IGNORECASE)\n",
        "                if next_match:\n",
        "                    next_section_start = section_start_index + next_match.start()\n",
        "                    break\n",
        "            # Extract the section content\n",
        "            section_content = resume_text[section_start_index:next_section_start].strip()\n",
        "            sections[section] = section_content\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "lGxDCHGLCgJm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def section_bifercation():\n",
        "  directory_path = '/content/drive/MyDrive/Colab Notebooks/resume_parser_test'  # Replace with your directory path\n",
        "  files = list_files_in_directory(directory_path)\n",
        "  experience_list=[]\n",
        "  for file in files:\n",
        "    # resume_text =extract_text_from_file('/content/drive/MyDrive/resume_parser/Charles Obuseh.pdf')\n",
        "    input_file_path='/content/drive/MyDrive/Colab Notebooks/resume_parser_test/'+str(file)\n",
        "    resume_text =extract_text_from_file(input_file_path)\n",
        "    # Identify sections in the resume\n",
        "    resume_sections = identify_sections(resume_text)\n",
        "    for section, content in resume_sections.items():\n",
        "        if section.upper()=='EXPERIENCE':\n",
        "          experience_list.append(content)\n",
        "        print(f\"--- {section.upper()} ---\")\n",
        "        print(content)\n",
        "        print()\n",
        "    print('*********************************************************************************')\n"
      ],
      "metadata": {
        "id": "saV8nFcfye5v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text):\n",
        "    # Define keywords or key phrases relevant to each section\n",
        "    skills_keywords = [\n",
        "        \"Python\", \"SQL\", \"Bash\", \"Java\", \"Scala\", \"HTML\", \"CSS\", \"JavaScript\", \"PL/SQL\", \"MATLAB\", \"R\",\n",
        "        \"Flask\", \"Git\", \"Pandas\", \"Tableau BI\", \"ETL\", \"Apache Spark\", \"Hadoop\", \"HDFS\", \"Snowflake Data Factory\",\n",
        "        \"SnowSQL\", \"Snowpipe\", \"Amazon Kinesis\", \"Firehose\", \"Databricks\", \"AWS\", \"EMR\", \"Athena\", \"Lambda\",\n",
        "        \"Amazon MW\", \"Apache Airflow\", \"AWS EC2\", \"QuickSight\", \"SNS\", \"Cloudera infrastructure\", \"Ambari\",\n",
        "        \"Hortonworks\", \"Sandbox\", \"Zeppelin Notebook\", \"RDS\", \"MySQL\", \"MongoDB\", \"DynamoDB\", \"SQLAlchemy\",\n",
        "        \"PostgreSQL\", \"S3 Buckets\", \"SQLite\", \"Azure\", \"Oracle\", \"Redshift\", \"ECS\", \"Docker\", \"Jenkins\",\n",
        "        \"Ubuntu\", \"CloudFormation\", \"CircleCI\", \"Microsoft Azure\", \"PowerApps\", \"Power BI\", \"Jupyter Hub\",\n",
        "        \"Analytic Development\", \"R-programming\", \"C#\", \"XML\", \"SAP Analytics Looker\", \"Google Charts\", \"Qlik\",\n",
        "        \"SSRS\", \"SSIS\", \"Informatica\", \"SAC\", \"JIRA\", \"Azure DevOps\", \"SharePoint\", \"MS-Teams\", \"Text comprehension\",\n",
        "        \"NLP\", \"Classification\", \"Pattern Recognition\", \"Imaging Recognition\", \"Detection\", \"Forecasting\", \"Clustering\",\n",
        "        \"Sentiment Analysis\", \"Predictive Analytics\", \"Decision Analytics\", \"Association Analysis\", \"Attribution modeling\",\n",
        "        \"Classification Regression Trees (CART)\", \"SVM\", \"Random Forest\", \"GBM\", \"PCA\", \"RNN\", \"NaÃ¯ve Bayes\",\n",
        "        \"Bayesian Analysis\", \"Statistical Inference\", \"Predictive Modeling\", \"Stochastic Modeling\", \"Linear Modeling\",\n",
        "        \"Behavioral Modeling\", \"NumPy\", \"Scikit-learn\", \"TensorFlow\", \"SciPy\", \"Matplotlib\", \"Seaborn\", \"Keras\", \"NLTK\",\n",
        "        \"Beautiful Soup\", \"StatsModels\", \"PyTorch\", \"OpenCV\", \"Sweetviz\", \"Theano\", \"Bokeh\", \"Plotly\", \"Genism\", \"Scrapy\",\n",
        "        \"kivy\", \"Caffe2\", \"Spyder\", \"RStudio\", \"Visual Studio\", \"A/B testing\", \"GitHub\", \"Agile methodologies\", \"Snowflake\",\n",
        "        \"MS Access\", \"Amazon S3\", \"Mongo DB\", \"My SQL\", \"Pyspark\", \"Communication\", \"Presentation skills\", \"Teamwork\",\n",
        "        \"Leadership\", \"Critical thinking\", \"Creativity\", \"Problem-solving\", \"Accountability\", \"Shell Scripting\", \"Storm\",\n",
        "        \"JSP\", \"Servlets\", \"Kimball data warehousing methodology\", \"Linear Regression\", \"Ridge Regression\", \"Polynomial Regression\",\n",
        "        \"Lasso Regression\", \"Elastic Net\", \"k-Means\", \"Hierarchical Clustering\", \"Latent Dirichlet Allocation (LDA)\",\n",
        "        \"Amazon Web Services\", \"GIT\", \"SVN\", \"CVS\", \"C++\", \"Arduino\", \"Libraries\", \"Dask\", \"Postgres\", \"Geopy\",\n",
        "        \"Geopandas\", \"Algorithms\", \"Deep Learning\", \"Time Series\", \"Mathematical Core\", \"Statistics\",\n",
        "        \"Probability\", \"Multivariate Calculus\", \"Optimization\", \"Functional\", \"Machine Learning\", \"Natural Language Processing (NLP)\",\n",
        "        \"Artificial Intelligence\", \"Business\", \"Collaborative\", \"Attention to Detail\", \"Enterprise UAV Systems Integration\",\n",
        "        \"Developing\", \"Distributed Enterprise-GIS Systems (ESRI, Open Source)\", \"ArcGIS Enterprise\", \"GeoEvent Server\",\n",
        "        \"ArcGIS ImageServer\", \"ArcGIS Notebook Server\", \"Geoserver\", \"Mapserver\", \"Linux SysAdmin\", \"Computer Vision\",\n",
        "        \"Pose Estimation\", \"Structure-from-Motion\", \"GIS 3D Reconstructions\", \"MVS\", \"Poisson Surface Reconstruction\",\n",
        "        \"Pipelines\", \"Geometric Deep Learning\", \"Point Clouds\", \"PointNet\", \"ShapeNet\", \"SyncSpecCNN\", \"YOLO3\",\n",
        "        \"Nvidia NERF implementations\", \"UAVs\", \"Databases\", \"SAP HANA\", \"PostGreSQL\", \"PostGIS\", \"Cassandra\",\n",
        "        \"HA Virtualization\", \"Hyper-V\", \"Xen\", \"ESXi\", \"AWS/Azure\", \"Containerization\", \"LXC\", \"Kubernetes\",\n",
        "        \"ProxMoxVE\", \"SSMS\", \"Sage\", \"IoT\", \"Virtual Machines\", \"GCP\", \"BigTable\", \"DataLab\"\n",
        "    ]\n",
        "    experience_keywords = [\n",
        "        \"Customer Solutions Data Engineer\", \"Systems and Data Engineer\", \"Consultant Measurements Field Engineer\",\n",
        "        \"Senior Technical Account Manager\", \"Engineering Coordinator\", \"COGNITIVE SCIENCE INSTITUTE Lab Manager\",\n",
        "        \"Postdoctoral Research Fellow\", \"Graduate Research Fellow\", \"Data Engineer KPMG, Tampa FL\",\n",
        "        \"Sr. Data Engineer Walmart, Sunnyvale CA\", \"Python Developer, Charter, MO\", \"Data Engineer ViacomCBS, NYC\",\n",
        "        \"Big Data Engineer WebSoc Technologies, Hyderabad India\", \"Data Engineer ARISSOFT SOLUTIONS, Hyderabad, India\",\n",
        "        \"Senior Data Engineer, Dhruvsoft Services Private Limited, Hyderabad, India\", \"Data Analyst, AI/ML Intern, Modak Analytics LLP, India\",\n",
        "        \"Data Engineer, Live Life Love Life Charity Foundation- Fundraising Coordinator, Vistara\",\n",
        "        \"ETL Developer, Charles Schwab\", \"Allstate, Associate Data Scientist Bangalore, INDIA Datakalp\",\n",
        "        \"Research Data Engineerin Intern Pittsburgh, USA Robert Bosch R&D\", \"Teaching Fellow New York City, New York University - Leonard N. Stern School of Business\",\n",
        "        \"New York\", \"Software Engineer Dubai, UAE Varal Group DMCC\", \"Data Engineer | TC Energy | Houston, TX\",\n",
        "        \"Odfjell Terminal | Seabrook, TX\", \"Enterprise, Products (Through CF) | Houston, TX\", \"Analytics Developer\",\n",
        "        \"Undergraduate Researcher\", \"Senior Data Analyst Tesoro Logistics (Through IG) | San Antonio, TX\", \"Business Intelligence Analyst\",\n",
        "        \"Systems Administrator\", \"Research Assistant\", \"Big Data Engineer\", \"Project Lead\", \"Data Scientist\",\n",
        "        \"Data Science Intern\", \"Data Analysis Research Associate\", \"Assistant System Administrator\", \"Freelance Developer\",\n",
        "        \"Note Taker\", \"Financial Systems and Data Intern\", \"Senior Analyst\", \"Senior Associate - Data Science & Quants Lab\",\n",
        "        \"Associate Software Engineer - Data Science & Quants Lab\", \"Clark Associates Inc Financial Systems and Data Intern\",\n",
        "        \"Dacapo Brokerage India Private Limited Senior Analyst\", \"Onsite Supply Chain Analytics Project - Doha, Qatar\",\n",
        "        \"DronaMaps Private Limited Founder & CEO / August 2016 to July 2022\", \"Sr. Database Developer\", \"Database and Business Intelligence Developer\",\n",
        "        \"HITACHI ENERGY Data Science Intern\", \"ENERGY INFORMATICS GROUP\", \"NATIONAL CENTRE IN BIG DATA & CLOUD COMPUTING Data Analysis Research Associate\",\n",
        "        \"Disaster Tweet Prediction\", \"Predict Churning Customers\", \"Climate Change Tracker\"\n",
        "    ]\n",
        "\n",
        "    # Find keywords in the text\n",
        "    keywords = {'skills': [], 'experience': [], 'projects': []}\n",
        "    for section, keywords_list in zip(keywords.keys(), [skills_keywords, skills_keywords, skills_keywords]):\n",
        "        for keyword in keywords_list:\n",
        "            if keyword.lower() in text:\n",
        "                keywords[section].append(keyword)\n",
        "\n",
        "    return keywords\n",
        "\n"
      ],
      "metadata": {
        "id": "co4YA0QegOth"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def keyword_in_file():\n",
        "#     directory_path = '/content/drive/MyDrive/Colab Notebooks/resume_parser_test'  # Replace with your directory path\n",
        "#     files = list_files_in_directory(directory_path)\n",
        "\n",
        "#     for file in files:\n",
        "#         input_file_path = os.path.join(directory_path, file)\n",
        "#         resume_text = extract_text_from_file(input_file_path)\n",
        "#         resume_sections = identify_sections(resume_text)\n",
        "#         for section, content in resume_sections.items():\n",
        "#             keywords = extract_keywords(content)\n",
        "#             with open(f'{section}_keywords.txt', 'a') as f:\n",
        "#                 f.write(f'Resume: {file}\\n')\n",
        "#                 section_keywords = keywords.get(section, [])  # Get keywords or an empty list if section is not found\n",
        "#                 f.write('\\n'.join(section_keywords) + '\\n')\n",
        "#                 f.write('--------------------\\n')\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "def keyword_in_file():\n",
        "    directory_path = '/content/drive/MyDrive/Colab Notebooks/resume_parser_test'  # Replace with your directory path\n",
        "    files = list_files_in_directory(directory_path)\n",
        "\n",
        "    for file in files:\n",
        "        input_file_path = os.path.join(directory_path, file)\n",
        "        resume_text = extract_text_from_file(input_file_path)\n",
        "        resume_sections = identify_sections(resume_text)\n",
        "        for section, content in resume_sections.items():\n",
        "            keywords = extract_keywords(content)\n",
        "            with open(f'{section}_keywords.txt', 'a') as f:\n",
        "                f.write(f'Resume: {file}\\n')\n",
        "                section_keywords = keywords.get(section, [])  # Get keywords or an empty list if section is not found\n",
        "                f.write('\\n'.join(section_keywords) + '\\n')\n",
        "                f.write('--------------------\\n')\n",
        "\n",
        "                # Display output on the screen\n",
        "                print(f'Resume: {file}')\n",
        "                print(f'--- {section.upper()} ---')\n",
        "                print('\\n'.join(section_keywords))\n",
        "                print('--------------------')\n",
        "\n",
        "keyword_in_file()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKTvgwdRvwBs",
        "outputId": "cca879f7-03b3-49fc-bdda-7f2ee939983b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume: Charles Obuseh.pdf\n",
            "--- EDUCATION ---\n",
            "\n",
            "--------------------\n",
            "Resume: Charles Obuseh.pdf\n",
            "--- EXPERIENCE ---\n",
            "Python\n",
            "SQL\n",
            "R\n",
            "ETL\n",
            "Amazon Kinesis\n",
            "Firehose\n",
            "Databricks\n",
            "AWS\n",
            "EMR\n",
            "Lambda\n",
            "Apache Airflow\n",
            "AWS EC2\n",
            "Cloudera infrastructure\n",
            "Ambari\n",
            "Hortonworks\n",
            "Sandbox\n",
            "Zeppelin Notebook\n",
            "Oracle\n",
            "Snowflake\n",
            "Pyspark\n",
            "--------------------\n",
            "Resume: Charles Obuseh.pdf\n",
            "--- SKILLS ---\n",
            "Python\n",
            "SQL\n",
            "Bash\n",
            "R\n",
            "Git\n",
            "Pandas\n",
            "Tableau BI\n",
            "ETL\n",
            "Apache Spark\n",
            "Hadoop\n",
            "HDFS\n",
            "Snowflake Data Factory\n",
            "SnowSQL\n",
            "Snowpipe\n",
            "Amazon Kinesis\n",
            "Firehose\n",
            "Databricks\n",
            "AWS\n",
            "EMR\n",
            "Athena\n",
            "Lambda\n",
            "Amazon MW\n",
            "Apache Airflow\n",
            "AWS EC2\n",
            "QuickSight\n",
            "SNS\n",
            "Cloudera infrastructure\n",
            "Ambari\n",
            "Hortonworks\n",
            "Sandbox\n",
            "Zeppelin Notebook\n",
            "RDS\n",
            "MySQL\n",
            "MongoDB\n",
            "DynamoDB\n",
            "SQLAlchemy\n",
            "PostgreSQL\n",
            "S3 Buckets\n",
            "SQLite\n",
            "Azure\n",
            "Oracle\n",
            "Snowflake\n",
            "Pyspark\n",
            "GIT\n",
            "Postgres\n",
            "Databases\n",
            "PostGreSQL\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Diw8hMPUdK4_",
        "outputId": "fa4bbd7f-7478-46c6-e822-98f18f2b59d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install find-job-titles\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDSBRxKKCfcU",
        "outputId": "0faff336-55b9-4477-cdd6-8cad8767c448"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: find-job-titles in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: acora in /usr/local/lib/python3.10/dist-packages (from find-job-titles) (2.4)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from find-job-titles) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from find_job_titles import Finder\n",
        "from collections import defaultdict\n",
        "\n",
        "list_of_job=defaultdict()\n",
        "def find_jobs():\n",
        "\n",
        "  finder = Finder()\n",
        "\n",
        "  directory_path = '/content/drive/MyDrive/Colab Notebooks/resume_parser_test'  # Replace with your directory path\n",
        "  files = list_files_in_directory(directory_path)\n",
        "  print(\"Files in directory:\")\n",
        "  for file in files:\n",
        "      print(file)\n",
        "      input_file_path='/content/drive/MyDrive/Colab Notebooks/resume_parser_test/'+str(file)\n",
        "      text=extract_text_from_file(input_file_path)\n",
        "      titles=finder.findall(text)\n",
        "      # print(\"Job titles found:\",titles)\n",
        "      matched_strings = [match.match for match in titles]\n",
        "      list_of_job[file]=matched_strings\n",
        "  #print(list_of_job)\n",
        "  return list_of_job"
      ],
      "metadata": {
        "id": "3RWg0OgaSVjj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_mapping():\n",
        "  f = open('/content/drive/MyDrive/Colab Notebooks/Onet.txt', 'r')\n",
        "  data = f.read()\n",
        "  # print(data)\n",
        "\n",
        "  codes = []\n",
        "  job_titles = []\n",
        "\n",
        "  # Split the data into lines\n",
        "  lines = data.strip().split('\\n')\n",
        "\n",
        "  # Extracting codes and job titles and appending them to respective lists\n",
        "  for line in lines:\n",
        "      # Splitting each line by whitespace to handle different delimiters\n",
        "      parts = line.split()\n",
        "      if len(parts) >= 2:\n",
        "          code = parts[0]\n",
        "          title = ' '.join(parts[1:])\n",
        "          codes.append(code)\n",
        "          job_titles.append(title)\n",
        "\n",
        "  # Creating a list of dictionaries\n",
        "  occupations_list = [{\"number\": code, \"title\": title} for code, title in zip(codes, job_titles)]\n",
        "\n",
        "  # Printing the list of dictionaries\n",
        "  #print(occupations_list)\n",
        "  f.close()\n",
        "  return codes,job_titles"
      ],
      "metadata": {
        "id": "CdEl3glbcxXA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "9QtzRmU5vJwv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "\n",
        "# Assuming get_average_vector function exists\n",
        "def get_average_vector(title, model):\n",
        "    vectors = [model.wv[word] for word in title if word in model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "MEpq0VTFXWrl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def job_mapping(job_titles,list_of_job):\n",
        "  # Preprocess job titles\n",
        "  job_titles_processed = [re.sub(r'[^\\w\\s]', '', title).lower().split() for title in job_titles]\n",
        "\n",
        "  mapped_jobs=[]\n",
        "\n",
        "  for word in list_of_job.values():\n",
        "      found = word\n",
        "      mapped = []\n",
        "      # Preprocess found values\n",
        "      found_processed = [re.sub(r'[^\\w\\s]', '', title).lower().split() for title in found]\n",
        "\n",
        "      # Train Word2Vec model\n",
        "      model = Word2Vec(sentences=job_titles_processed, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "      mapped_titles = set()  # Keep track of mapped titles to ensure uniqueness\n",
        "      similarity_threshold = 0.5  # Set similarity threshold\n",
        "\n",
        "      for title in found_processed:\n",
        "          max_similarity = -1\n",
        "          most_similar_title = None\n",
        "          title_vector = get_average_vector(title, model)\n",
        "          if title_vector is not None:\n",
        "              for job_title in job_titles_processed:\n",
        "                  job_title_vector = get_average_vector(job_title, model)\n",
        "                  if job_title_vector is not None:\n",
        "                      similarity = cosine_similarity(title_vector, job_title_vector)\n",
        "                      if similarity > max_similarity:\n",
        "                          max_similarity = similarity\n",
        "                          most_similar_title = job_title\n",
        "              if max_similarity >= similarity_threshold:\n",
        "                  mapped.append(' '.join(most_similar_title))\n",
        "                  mapped_titles.add(tuple(most_similar_title))  # Convert list to tuple\n",
        "              else:\n",
        "                  mapped.append(' '.join(title))  # Retrieve the original title\n",
        "          else:\n",
        "              mapped.append(' '.join(title))  # Retrieve the original title\n",
        "\n",
        "  #    print(mapped)\n",
        "      mapped_jobs.append(mapped)\n",
        "  i=0\n",
        "  for word in list_of_job.values():\n",
        "    print(\"Extracted job roles:\",word)\n",
        "    print(\"Mapped job roles:\",mapped_jobs[i])\n",
        "    i+=1\n",
        "    print(\"---------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "ab45QCvyxy-h"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "section_bifercation()\n",
        "keyword_in_file()\n",
        "list_of_jobs = find_jobs()\n",
        "codes,job_titles = pre_mapping()\n",
        "job_mapping(job_titles,list_of_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2pS-5XxyVDJ",
        "outputId": "1f64f172-22a1-4532-ef7d-6ed35173168f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- EDUCATION ---\n",
            "& certifications university of north texas | master of science | mechanical and energy engineering | denton, tx rice university | certificate | data analytics | houston, tx license licensed texas professional engineer | texas board of professional engineer and land surveyors additional\n",
            "\n",
            "--- EXPERIENCE ---\n",
            "data engineer | tc energy | houston, tx | 09/2019 â€“ present | odfjell terminal | seabrook , tx | 02/2019 â€“ 09/2019 | enterprise products (through cf) | houston , tx | 10/2018 â€“ 02/2019 â€¢ design and implementation of process data pipeline for asset data migration into aws cloud environment â€¢ built snowflake data pipeline using amazon kinesis firehose starting from the aws ec2 logs to storage in snowflake and aws s3 bucket post -transformation and orchestrating through amazon managed workflows for apache airflow (mwaa) dags .. programming language used are pyspark, python, sparksql â€¢ architecting data pipeline with cloudera infrastructure â€“ ambari, hortonworks sandbox using zeppelin notebook . programming language used are python, sql â€¢ created datastream using aws kinesis datastream and kinesis firehose . programming language used are pyspark, python, sparksql â€¢ migration solution from sap and maximo to data lake. â€¢ solved data quality issues using aws databrew and apache pyspark on aws emr and databricks on a ws ec2 clusters for data wrangling and transformations. â€¢ aws project monitoring using aws lambda and aurora â€¢ performed sql data analysis using oracle database â€¢ regular meeting with stakeholders to gather user requirements and ascertain objectives. senior data analyst | tesoro logistics (through ig) | san antonio , tx | 04/2017 â€“ 10/2018 â€¢ design and implementation of process data pipeline and loading of structured data for performing quantitative and qualitative asset risk modelling, predictive analysis for program budgeting and forecast. â€¢ created process safety risk reduction metrics projec t engineer | weatherford | houston , tx | 09/2013 â€“ 03/201 6 â€¢ performed etl of drilling well bottom hole pressure â€˜.mbdâ€™ data from field and wireline instrumentations (bottom hole pressure, well depth and true vertical height of drilling bit). â€¢ cleaned and transformed data into a â€˜.csvâ€™ data file. load data into weatherford microflux database for analysis. â€¢ project management of secure drilling operations of over 80 wells in canada, iraq, nigeria, angola, and cameroon for clients such as repsol, shell, exxon mobile, chevron and total\n",
            "\n",
            "--- SKILLS ---\n",
            ". i am proficient in technical software, languages, and tools with the ability to learn new tools, databases and systems to maintain/enhance strategic vi sion of the organization and able to provide cutting -edge data engineering skills. seeking a reputable organization to contribute to with opportunities for personal growth. . technical skills languages â€¢ python, sql others â€¢ git, bash data manipulation & visualization â€¢ pandas, tableau bi, etl, apache spark, hadoop hdfs, snowflake data factory, snowsql and snowpipe , amazon kinesis firehose , databricks, aws emr , athena, lambda , amazon mw apache airflow and aws ec2 quicksight and sns, cloudera infrastructure â€“ ambari, hortonworks sandbox and zeppelin notebook databases and storage â€¢ aws rds, mysql, mongodb, aws dynamodb, sqlalchemy, postgresql, aws s3 buckets, sqlite, azure , oracle. career experience data engineer | tc energy | houston, tx | 09/2019 â€“ present | odfjell terminal | seabrook , tx | 02/2019 â€“ 09/2019 | enterprise products (through cf) | houston , tx | 10/2018 â€“ 02/2019 â€¢ design and implementation of process data pipeline for asset data migration into aws cloud environment â€¢ built snowflake data pipeline using amazon kinesis firehose starting from the aws ec2 logs to storage in snowflake and aws s3 bucket post -transformation and orchestrating through amazon managed workflows for apache airflow (mwaa) dags .. programming language used are pyspark, python, sparksql â€¢ architecting data pipeline with cloudera infrastructure â€“ ambari, hortonworks sandbox using zeppelin notebook . programming language used are python, sql â€¢ created datastream using aws kinesis datastream and kinesis firehose . programming language used are pyspark, python, sparksql â€¢ migration solution from sap and maximo to data lake. â€¢ solved data quality issues using aws databrew and apache pyspark on aws emr and databricks on a ws ec2 clusters for data wrangling and transformations. â€¢ aws project monitoring using aws lambda and aurora â€¢ performed sql data analysis using oracle database â€¢ regular meeting with stakeholders to gather user requirements and ascertain objectives. senior data analyst | tesoro logistics (through ig) | san antonio , tx | 04/2017 â€“ 10/2018 â€¢ design and implementation of process data pipeline and loading of structured data for performing quantitative and qualitative asset risk modelling, predictive analysis for program budgeting and forecast. â€¢ created process safety risk reduction metrics projec t engineer | weatherford | houston , tx | 09/2013 â€“ 03/201 6 â€¢ performed etl of drilling well bottom hole pressure â€˜.mbdâ€™ data from field and wireline instrumentations (bottom hole pressure, well depth and true vertical height of drilling bit). â€¢ cleaned and transformed data into a â€˜.csvâ€™ data file. load data into weatherford microflux database for analysis. â€¢ project management of secure drilling operations of over 80 wells in canada, iraq, nigeria, angola, and cameroon for clients such as repsol, shell, exxon mobile, chevron and total\n",
            "\n",
            "*********************************************************************************\n",
            "Resume: Charles Obuseh.pdf\n",
            "--- EDUCATION ---\n",
            "\n",
            "--------------------\n",
            "Resume: Charles Obuseh.pdf\n",
            "--- EXPERIENCE ---\n",
            "Python\n",
            "SQL\n",
            "R\n",
            "ETL\n",
            "Amazon Kinesis\n",
            "Firehose\n",
            "Databricks\n",
            "AWS\n",
            "EMR\n",
            "Lambda\n",
            "Apache Airflow\n",
            "AWS EC2\n",
            "Cloudera infrastructure\n",
            "Ambari\n",
            "Hortonworks\n",
            "Sandbox\n",
            "Zeppelin Notebook\n",
            "Oracle\n",
            "Snowflake\n",
            "Pyspark\n",
            "--------------------\n",
            "Resume: Charles Obuseh.pdf\n",
            "--- SKILLS ---\n",
            "Python\n",
            "SQL\n",
            "Bash\n",
            "R\n",
            "Git\n",
            "Pandas\n",
            "Tableau BI\n",
            "ETL\n",
            "Apache Spark\n",
            "Hadoop\n",
            "HDFS\n",
            "Snowflake Data Factory\n",
            "SnowSQL\n",
            "Snowpipe\n",
            "Amazon Kinesis\n",
            "Firehose\n",
            "Databricks\n",
            "AWS\n",
            "EMR\n",
            "Athena\n",
            "Lambda\n",
            "Amazon MW\n",
            "Apache Airflow\n",
            "AWS EC2\n",
            "QuickSight\n",
            "SNS\n",
            "Cloudera infrastructure\n",
            "Ambari\n",
            "Hortonworks\n",
            "Sandbox\n",
            "Zeppelin Notebook\n",
            "RDS\n",
            "MySQL\n",
            "MongoDB\n",
            "DynamoDB\n",
            "SQLAlchemy\n",
            "PostgreSQL\n",
            "S3 Buckets\n",
            "SQLite\n",
            "Azure\n",
            "Oracle\n",
            "Snowflake\n",
            "Pyspark\n",
            "GIT\n",
            "Postgres\n",
            "Databases\n",
            "PostGreSQL\n",
            "--------------------\n",
            "Files in directory:\n",
            "Charles Obuseh.pdf\n",
            "Extracted job roles: ['Data Engineer', 'Data Engineer', 'Senior Data Analyst', 'Energy Engineer', 'Professional Engineer', 'Professional Engineer', 'Land Surveyor']\n",
            "Mapped job roles: ['data scientists', 'data scientists', 'data scientists', 'energy auditors', ['professional', 'engineer'], ['professional', 'engineer'], ['land', 'surveyor']]\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rh2yFP5Ax6RI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNR-nUUfFxDV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLW37b1NGImq"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}